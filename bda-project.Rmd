---
title: "BDA - project"
output: html_notebook
---

# 1. Introduction

It is generally known by railway traffic managers (and the public) that bad weather is one major reason for widespread rail traffic disruption. While several different weather situations affects to rail traffic, especially long lasting snow storms are known to be very problematic for the rail traffic.

This project tries to analyse rail traffic disruptions caused by bad weather. Only few previous similar attemps exists so far but the results are encouringing. Ludvigsen & Klæboe have studied affects of winter weather to the rail traffic in Norway, Sweden, Finland, Switcherland and Poland [1]. They have found that cold temperature together with heavy snow precipitation explains from 60 to 80 percent of all delays. Oneton & co. have created a train delay prediction caused by bad weather in Northern Italy [2]. Their method was based on Random Forest Regression (RFR) and features consists of air temperature, humidity, wind direction and spped, precipitation amount, pressure and sun radiation. 

Train network is naturally very interconnected system and delay of one train often affects to several others. Here, we anyway consider only effects of weather and leave these dependecies out. This keeps things much more simplier but of course cause some 'noise' to the data.
First some necessary initialisation tasks

```{r message=FALSE, warning=FALSE, include=FALSE}
#Sys.setenv(https_proxy="http://wwwproxy.fmi.fi:8080")
install.packages("brms")
install.packages("tidyverse")
install.packages("anytime")
install.packages("caTools")
```

```{r message=FALSE, warning=FALSE, include=FALSE}
install.packages("ggplot2")
```

```{r message=FALSE, warning=FALSE, include=FALSE}
install.packages("brms")
```

```{r message=FALSE, warning=FALSE}
require(lubridate)
library(anytime)
library(ggplot2)
library(brms)
theme_set(theme_minimal())
```


# 2. Description of the data, and the analysis problem

For the task we need train delay information among with weather data. Train delay information is provided by Finnish Transport Agency (FTA). Online data is available at https://digitrafic.com. In our case, we use archive data got directly from FTA. Our dataset has been pre-processed so that train information is reported with one hour interval. The dataset contains trains delays reported on every hour on every station. Delays are reported as sum of delays of all trains arrived to the station during the particular hour. The dataset contain additional delays between train stations ('delay') and cumulated delays at the end station ('total_delay'). Here, we use additional delays.

Finnish Meteorological Institute (FMI) provides weather observations. The data is available online (see https://en.ilmatieteenlaitos.fi/open-data for more details). Our data fetched from the online system and stored separately. The data contains aggregation of weather observations withing 100 kilometers and particular hour from all trains stations and times when trains have arrived to the station.

Our archive contain data from 2010 to 2018. Train delay data and weather data have been concatted to the one single dataframe. The whole dataset contains over 27 million rows which is beyond a scope of this project. Thus we use only subset of the data. For training purposes, we use only Helsinki Railway station data For validation purposes, we use 10 days period from 1st Jan to 10th Jan 2011. 

More detailed description of the parameters are listed below

time:                start time of one hour interval
trainstation:        train station short code
train_type:          K -> inter city
                     L -> commuter
                     T -> cargo
                     M -> other
train_count:         amount of trains passed the station during the particular hour
total_delay:         amount of delay at the end station of the train
delay:               amount of delay between previous and current station
name:                observation station name
lat:                 train station latitude
lon:                 train station longitude
pressure:            air pressure (hPa)
max_temperature:     maximum temperature during the time interval (C)
min_temperature:     minimum temperature (C)
mean_temperature:    mean temperature (C)
mean_dewpoint:       mean dew point (C)
mean_humidity:       mean humidity (percents)
mean_winddirection:  mean wind direction (degrees)
mean_windspeedms:    mean wind speed (m/s), 
max_windgust:        maximum wind gust (m/s)
max_precipitation1h: 1 hour precipitation accumulation (mm)
max_snowdepth:       maximum snow depth (cm)
max_n:               maximum cloudiness (1/8)
min_vis:             minimum visibility (m) 
min_clhb:            minimum cloud base height (m)
max_precipitation3h: 3 hour precipitation accumulation (mm)
max_precipitation6h: 6 hour precipitation accumulation (mm)
flashcount:          flash count with 30 km radius from the train station

- 99 stands for missing value

Data license is CC4BY.

We load data from csv file following:

```{r}
data_file <- "hki_all.csv"
val_data_file <- "validation.csv"

df <- read.csv(data_file)
df$datetime <- anytime::anytime(df$time)
```

Loaded dataframe is arranged by time. It looks like following. Now there's 99014 rows of data.

```{r}
head(df)
tail(df)
dim(df)
```

Timeseries of the delay is plotted below. One can see, that the timeseries is very spiky which is expected. Normally trains run relatively well on schedule and when bad delays happen, disruptions are typically widespread accross the rail network. One may also wonder high base value of the delays (roughly 100 minutes). It's good to remember that values are sum of delays of all trains arrived to all stations. The plot also shows that dataset contains lots of missing data. 

```{r}
library(scales)
options(repr.plot.width=10, repr.plot.height=4)
ggplot(data = df, aes(x = datetime, y = delay))+
  geom_line(color = "#00AFBB", size = 1) +
  scale_x_datetime(date_labels = "%a %d %b %Y") +
  theme(axis.text.x=element_text(angle=45, hjust=1))
```

As weather observation network is relatively sparse (around 200 weather stations) and most stations measure only subset of all used variables, the data contain lots of missing values (encoded with -99). In this case we want to exclude them in the future processing. After this we get 41 255 rows of data which is used for fitting the model.

```{r}
df[df == -99] <- NA
df <- df[complete.cases(df),]
```

To get sense of data, we can print histograms of measurements. We can also draw histograms of parameters for all rows where delay between stations have been significant, let's say over 50 minutes. This way we can compare cases where rail have travelled in time and where bad delays have occured.

Resulting graph is plotted below. Blue bars stands for all rows and red columns for rows with significant delays. Most histograms looks roughly following normal distribution. Histogram of delays follows the same theme: most of trains run in time but the distribution has a very long tail. Total cloudiness ('max_n') can get discrete values from 0 to 8. 9 stands for 'undetermined'.

From the plot we can see that at least temperature, snow depth, visibility and wind direction has been significantly different when severe rail traffic disruptions has been occurred. So we use these parameters to fit our model. We also add wind gust as it's assumed to cause train delays (specially with drifting snow). 


```{r}
library(reshape2)

cols = c('train_count', 'delay', 'total_delay', 
         'min_temperature', 'mean_temperature', 
         'max_temperature', 'pressure', 'mean_humidity', 
         'mean_winddirection', 'mean_windspeedms', 'max_windgust', 
         'max_precipitation1h', 'max_snowdepth', 'max_n', 
         'min_vis', 'min_clhb', 'max_precipitation3h', 
         'max_precipitation6h')

df_delay <- df[df$delay > 50,]

par(mfrow=c(2,3))
options(repr.plot.width=10, repr.plot.height=6)

for (col in cols){
    p1 <- hist(df[,col],
          main=paste("Histogram of ",col),
          xlab='', col=rgb(0,0,1), border=rgb(0,0,1),
          breaks=50, freq=FALSE)
    p2 <- hist(df_delay[,col],
          main=paste("Histogram of ",col),
          xlab='', col=rgb(1,0,0,1/4), border=rgb(1,0,0,1/2),
          breaks=50, freq=FALSE, add=T)
}
```


# 3. Description of the model


```{r Non-informative prior}
options(mc.cores = parallel::detectCores())
fit1 <- brm(
    delay ~ 1 + min_temperature + max_snowdepth +  max_windgust + mean_winddirection + min_vis,
    data = df
)
```


```{r}
summary(fit1)
```


# 4. Description of the prior choices

Non informative

```{r}
prior_summary(fit1)
```

# 5. Stan
Stan code is generated by BRMS package. It can be printed following
```{r}
fit1$model
```

# 6. How the Stan model run


# 7. Convergence diagnostics

To analyse model convergence, we plot variables and check RHat values and effective sample size. 

For weak prior, RHat stays under 1.05 so the model has converged well.

```{r}
rhat(fit1)
stanplot(fit1, type='rhat')
```


To check a convergence of different chains and posterior distribution, we can plot fitted models. We can see, that all chains have converged to similar values for all parameters. We can also see, that all parameters have quite similar posterior distribution (with different means of course).  

```{r}
plot(fit1)
```


To analyse effective sample size, we can plot ratio of effective samples and all samples. This means amount of draws being able to estimate true mean value of the parameter. We can see that none Neff/N values are below 0.1 and pretty large amount of them are over 0.5, which again verify that both models have converged well.

```{r}
stanplot(fit1, type='neff')
```

As conclusion, the models have converged well.

# 8. Posterior predictive checking

In posterior predictive checking one compares modelled and observed data. In other words, we take draws from the model and compare them with observed data. This way we can find systematic differenced which could indicate that modelling has failed somehow.

```{r}
library("bayesplot")
yrep <- posterior_predict(fit1, draws = 500)
```


```{r}
dim(yrep)
dim(df[complete.cases(df),])
```

First we look plain mean value of the simulations (light blue) and the observed data (dark blue). Average of observed data is in the middle of the simulated distribution, so everything looks fine.

```{r}
y <- df$delay
```


```{r}
ppc_stat(y, yrep, binwidth=.005)
```

If we look probability that delay is 0 minutes in the simulations (light blue) and the observed data (dark blue), we can see a slight difference. 

```{r}
zero <- function(x) mean(x == 0)
```


```{r}
ppc_stat(y, yrep, 'zero', binwidth=.005)
```

Similarly, if we look sever disrubtions, probability that delay is over 50 minutes, in the simulations (light blue) and the observed data (dark blue), we can see a slight difference. The difference is anyway smaller than in the probability of 0 minute delay.  

```{r}
over50 <- function(x) mean(x > 50)
```


```{r}
ppc_stat(y, yrep, 'over50', binwidth=.005)
```

We can analyse a distribution of simulated draws and observed data by plotting them into the same plot. We use first 1000 rows from the data to avoid running out of memory. Resulting image tells us that our models are not able to follow true shape of observations very well. They produce negative values and miss large amount of 0 values. 


```{r}
ppc_dens_overlay(y[0:1000], yrep[,0:1000])
```

# 9. Model comparison
We try model with fewer variables (predictors)

```{r Fewer parameter}
options(mc.cores = parallel::detectCores())
fit2 <- brm(
    delay ~ 1 + min_temperature + max_snowdepth + 
            mean_winddirection + 
            max_precipitation6h,
    data = df,
    # prior = prior1
)
```


```{r}
loo <- loo(fit1,fit2)
```



# 10. Predictive performance assessment

To do predictive performance assesment, we need to read test data which our model hasn't seen yet. We use similar 10 days set from beginning of 2011.

```{r}
validation <- read.csv(val_data_file)
validation$datetime <- anytime::anytime(validation$time)
validation <- validation[order(validation$datetime),]
```

Then we create a prediction of train delays based on the same parameters we have used to fit our model.  

## Model 1

```{r}
val_data <- validation[,c('min_temperature', 'pressure', 'mean_humidity', 'max_windgust', 'mean_winddirection', 'max_precipitation6h', 'trainstation')]
pred <- predict(fit1, newdata = val_data, re_formula = NA)
head(pred, 3)
```

We can plot values and corresponding estimated errors. We can see that model forecasts negative values which of course doesn't make sense. Median values see plausible but estimated errors are large. From the plot we can also see, that predicted values are very discrete which is not good.

```{r}
options(repr.plot.width=4, repr.plot.height=4)
summary(pred)
plot(pred)
```

To really visualise predictions, we plot predicted delays and true delays on the same plot. We select Helsinki Railwaystation for this illustration. From the plot we can see, that although the model has converged well, it has no real prediction power. 

```{r}
val_data_hki <- validation[validation$trainstation == 'HKI' ,c('min_temperature', 'pressure', 'mean_humidity', 'max_windgust', 'mean_winddirection', 'max_precipitation6h', 'trainstation')]
pred_hki <- predict(fit1, newdata = val_data_hki, re_formula = NA)
head(pred_hki, 3)

options(repr.plot.width=14, repr.plot.height=4)
val_times_hki <- validation[validation$trainstation == 'HKI', c('datetime','delay')]
fitval_hki <- data.frame(cbind(val_times_hki,pred_hki[,-2]))
names(fitval_hki) <- c("time", "delay", "estimate", "lower", "upper")
head(fitval_hki, 3)
ggplot(data=fitval_hki, aes(x = time, y = 'delay')) +
    geom_line(data = fitval_hki, aes(y = delay), size = 0.5) +
    geom_line(data = fitval_hki, aes(y = lower), size = 0.11, col='#00AFBB') +
    geom_line(data = fitval_hki, aes(y = upper), size = 0.11, col='#00AFBB') +
    geom_line(data = fitval_hki, aes(y = estimate), size = 0.5, col='#00AFBB')
```

We can calculate model performance in terms of loss functions. In particular, we use RMSE and MAE to describe the performance. 

```{r}
rmse <- function(error)
{
    sqrt(mean(error^2))
} 
mae <- function(error)
{
    mean(abs(error))
}
error <- validation[,'delay'] - fitval[,'estimate']
cat(sprintf("RMSE: %.2f\n", rmse(error)))
cat(sprintf("MAE:  %.2f", mae(error)))
```


------------ 
## Model 2


```{r}
val_data <- validation[,c('min_temperature', 'min_vis', 'max_windgust', 'mean_winddirection', 'max_snowdepth')]
pred <- predict(fit2, newdata = val_data, re_formula = NA)
head(pred, 3)
```

We can plot values and corresponding estimated errors. We can see that model forecasts negative values which of course doesn't make sense. Median values see plausible but estimated errors are large. From the plot we can also see, that predicted values are very discrete which is not good.

```{r}
options(repr.plot.width=4, repr.plot.height=4)
summary(pred)
plot(pred)
```

To really visualise predictions, we plot predicted delays and true delays on the same plot. We select Helsinki Railwaystation for this illustration. From the plot we can see, that although the model has converged well, it has no real prediction power. 

```{r}
val_data_hki <- validation[validation$trainstation == 'HKI' ,c('min_temperature', 'min_vis', 'max_windgust', 'mean_winddirection', 'max_snowdepth')]
pred_hki <- predict(fit2, newdata = val_data_hki, re_formula = NA)
head(pred_hki, 3)

options(repr.plot.width=14, repr.plot.height=4)
val_times_hki <- validation[validation$trainstation == 'HKI', c('datetime','delay')]
fitval_hki <- data.frame(cbind(val_times_hki,pred_hki[,-2]))
names(fitval_hki) <- c("time", "delay", "estimate", "lower", "upper")
head(fitval_hki, 3)
ggplot(data=fitval_hki, aes(x = time, y = 'delay')) +
    geom_line(data = fitval_hki, aes(y = delay), size = 0.5) +
    geom_line(data = fitval_hki, aes(y = lower), size = 0.11, col='#00AFBB') +
    geom_line(data = fitval_hki, aes(y = upper), size = 0.11, col='#00AFBB') +
    geom_line(data = fitval_hki, aes(y = estimate), size = 0.5, col='#00AFBB')
```

We can calculate model performance in terms of loss functions. In particular, we use RMSE and MAE to describe the performance. 

```{r}
rmse <- function(error)
{
    sqrt(mean(error^2))
} 
mae <- function(error)
{
    mean(abs(error))
}
error <- validation[,'delay'] - fitval[,'estimate']
cat(sprintf("RMSE: %.2f\n", rmse(error)))
cat(sprintf("MAE:  %.2f", mae(error)))
```


# 11. Sensitivity analysis

We test with different informative prior

```{r setup weakly informative prior}
prior1 <- c(set_prior("normal(-0.625,45)", class = "b", coef="min_temperature"),
           set_prior("normal(1024,69)", class = "b", coef="max_snowdepth"),
           set_prior("normal(4,9)", class = "b", coef="max_windgust"),
           set_prior("normal(1024,69)", class = "b", coef="mean_winddirection"),
           set_prior("normal(85,17.5)", class = "b", coef="min_vis"),
           set_prior("normal(10,100)", class = "Intercept"))
```


```{r Model with weakly Informative prior}
options(mc.cores = parallel::detectCores())
fit3 <- brm(
    delay ~  1 + min_temperature + max_snowdepth + 
             max_windgust + 
            mean_winddirection + min_vis
    data = df,
    prior = prior1
)
```


```{r setup highly informative prior}
prior1 <- c(set_prior("normal(-0.625,45)", class = "b", coef="min_temperature"),
           set_prior("normal(1024,69)", class = "b", coef="max_snowdepth"),
           set_prior("normal(4,9)", class = "b", coef="max_windgust"),
           set_prior("normal(1024,69)", class = "b", coef="mean_winddirection"),
           set_prior("normal(85,17.5)", class = "b", coef="min_vis"),
           set_prior("normal(10,100)", class = "Intercept"))
```


```{r Model with highly Informative prior}
options(mc.cores = parallel::detectCores())
fit3 <- brm(
    delay ~  1 + min_temperature + max_snowdepth + 
             max_windgust + 
            mean_winddirection + min_vis
    data = df,
    prior = prior1
)
```

Highly informative prior is far from the observed data --> highly influence the posterior results

# 12. Discussion of problems, and potential improvements

The model ignore the time-series structure of the data, treating them as thousands of independent observations
The model is not good for predictive performance. In other previous research with same data, it show that tree model such that random forest performs better than linear regression.

Potential improvements maybe how to include Bayesian inference into complex model such that random forest


# References

[1] Ludvigsen, J., & Klæboe, R. (2014). Extreme weather impacts on freight railways in Europe. Natural Hazards. https://doi.org/10.1007/s11069-013-0851-3

[2] Oneto, L., Fumeo, E., Clerico, G., Canepa, R., Papa, F., Dambra, C., … Anguita, D. (2016). Advanced analytics for train delay prediction systems by including exogenous weather data. In Proceedings - 3rd IEEE International Conference on Data Science and Advanced Analytics, DSAA 2016 (pp. 458–467). https://doi.org/10.1109/DSAA.2016.57

